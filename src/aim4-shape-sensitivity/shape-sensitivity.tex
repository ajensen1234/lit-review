\subsection{Introduction}

\begin{itemize}
  \item Shape descriptors - used to provide metrics to talk about shape. Invariant shape descriptors, vs variant (size, etc)
  \item Model image registration is fundamentally about shape
  \item Some shapes might be better than others, need to take a first principles approach
  \item Talk about symmetry traps, where multiple positions produce the same projective geometry
  \item Want to explore the relative sensitivity of the projections of shape to their input 3D geometry and the 3D geometry itself
\end{itemize}

The application of Joint Track Machine Learning to reverse Total Shoulder Arthroplasty (rTSA) implants has unfolded a complex scenario.
It highlights the intricate relationship between optimization algorithm performance, specific cost functions, and the inherent shape of the 3D models used for registration.
A particular challenge lies in the intrinsic properties of humeral models, which present unique difficulties in model-image registration.
To address these challenges, a thorough understanding of the relationship between a 3D shape and its projective geometry is crucial.
This understanding is expected to shed light on the stark differences in algorithmic performance between total knee arthroplasty (TKA) and rTSA implants.


Understanding shape and it's salient features has been a crucial aspect of computer vision since it was intertwined with psychology and neurology \cite{attneaveInformationalAspectsVisual1954,attneaveQuantitativeStudyShape1956}.
Many intuitively appealing ideas about shape, such as the salience of curvature and vanishing points in projections, required mathematical definition to be effectively incorporated into image processing algorithms.
Invariant Shape Descriptors, which remain consistent across rigid transformations or scaling, are particularly significant \cite{zhangReviewShapeRepresentation2004}.
These descriptors encapsulate the essence of a shape, independent of factors like rotation, scaling, or position in an image.
Normalized Fourier Descriptors are among the most notable examples of invariant shape descriptors that have been used for aircraft recognition \cite{wallaceEfficientThreedimensionalAircraft1980,wallaceAnalysisThreedimensionalMovement1980,richardIdentificationThreeDimensionalObjects1974}, aerial photography classification \cite{linClassificationPartial2D1987}, model-image registration \cite{zossoBiplanar2Dto3DRegistration2008}, and even measuring TKA kinematics from single-plane images \cite{banksAccurateMeasurementThreedimensional1996}.
Hu moments \cite{huVisualPatternRecognition1962}, the Hough Transform \cite{ballardGeneralizingHoughTransform1981}, Shape Context \cite{belongieShapeMatchingObject2002}, Curvature scale space \cite{koenderinkSurfaceShapeCurvature1992}, the Angular Radial Transform \cite{leeNewShapeDescription2012}, and multi-scale Shape Descriptors \cite{al-thelayaInShaDeInvariantShape2021} have all been proposed as robust methods for vectorizing a shape into mathematically comparable elements.

The central inquiry of this chapter is whether a robust binary shape descriptor can elucidate the relative underperformance of model-image registration for rTSA implants compared to TKA implants. This question not only addresses a specific technical challenge but also aims to contribute to the broader understanding of shape analysis in medical imaging.

\subsection{Methods}
\subsubsection{Data Collection}
First, we collected one manufacturer-provided model from each of: rTSA humeral implant, rTSA glenosphere implant, TKA femoral implant, and TKA tibial implant for testing shape sensitivity.
\subsubsection{Image Generation}
The binary silhouette of each implant was rendered using an in-house CUDA camera model (CUDA Version 12.1) \cite{nickollsScalableParallelProgramming2008} to a $1024\times 1024$ image plane.
The focal length of the pinhole camera model was 1000mm and each pixel was 0.3mm.
All CUDA programming was performed on an NVIDIA Quadro P2200 GPU.
\subsubsection{Shape Descriptor and Shape Differences}
Due to the our desire to explore the sensitivity of rotational orientation to the shape sensitivity, we choose the Invariant Angular Radial Transform Descriptor (IARTD) as the shape descriptor, because this specifically addresses the radial aspects of the problem, ensuring strict invariance with respect to in-plane rotation \cite{leeNewShapeDescription2012}.
For clarity in representation, successive rotations were denoted as subscripts, such that $R_{z}R_{x}R_{y} = R_{z,x,y}$. The application of the IARTD equation to an implant at a specific input orientation $R_{z,x,y}$ was represented as $IARTD(R_{z,x,y})$.

Shape differences were calculated using the central difference equation on the IARTD vector produced from two different orientations.
The grid of starting orientations was $\pm 30$ with a step size of $5$ for each of the $x$, $y$, and $z$ axes.
The ``differences'' along each axes were computed by applying a positive and negative rotation ($\pm \delta $) of 1 degree.
And so, for every input $x,y,z$ rotation, there will be three shape differences, one for each $\delta_{x}$, $\delta_{y}$ or $\delta_{z}$ (\cref{eq:shape-derivative}).



\begin{equation}
  \label{eq:shape-derivative}
    \Delta IARTD(R_{z,x,y}) = IARTD(R_{z,x,y,+\delta}) - IARTD(R_{z,x,y,-\delta})
\end{equation}

Because each element of the IARTD vector is at a different scale, we must standardize each element in order to ensure accurate assessment of global behavior without analysis being dominated by a single value.
We use z

\subsubsection{Shape Sensitivity}


\subsubsection{Code Examples}

\begin{lstlisting}
  // IARTD.cpp
std::vector<float> calculateIARTD(img_desc* img_desc_gpu,
                                  gpu_cost_function::GPUImage* dev_image) {
    /**
     * This is a function to calculate the Invariant Angular Radtial Transform
     Descriptor.
     * See: J.-M. Lee and W.-Y. Kim, A New Shape Description Method Using
     Angular Radial Transform, IEICE Trans. Inf. \& Syst., vol. E95.D, no. 6,
     pp. 1628-1635, 2012, doi: 10.1587/transinf.E95.D.1628.
     * The input is a binary image (either a segmentation or a projected image),
     and the output is the vector containing the descriptor variables.
     */
    const int MAX_P =
        8;  // Setting max values for the number of "rings" and "angles"
    const int MAX_N = 3;
    float phase_n_1[MAX_N + 1];  // Creating array for phase correction term
    // (Eqs 15, 16)
    int H = img_desc_gpu->height();
    int W = img_desc_gpu->width();
    std::vector<float> iartd(2 * (MAX_N + 1) * (MAX_P + 1));

    auto idx = [](int n, int p) -> int {
        return (n * MAX_P + p - 1) * 2;
    };  // Lambda for easy indexing
    for (int n = 0; n <= MAX_N; n++) {
        for (int p = 0; p <= MAX_P; p++) {
            std::complex<float> fnp = img_desc_gpu->art_n_p(n, p, dev_image);
            if (p > 1) {
                iartd[idx(n, p)] = abs(fnp) / (float)(H * W);
                iartd[idx(n, p) + 1] = arg(fnp) / (float)(H * W);

            } else if (p == 1) {
                // But, we want to keep values at p=1 for the normalization
                // procedure
                phase_n_1[n] = arg(fnp);
            }
        }
    }
    // Phase Correction using values from p = 1 (Eq 15, 16)
    for (int n = 0; n <= MAX_N; n++) {
        for (int p = 2; p <= MAX_P; p++) {
            std::complex<float> fnp_prime =
                std::complex<float>(iartd[idx(n, p)], iartd[idx(n, p) + 1]) *
                exp(std::complex<float>(0.0, -p * phase_n_1[n]));
            iartd[idx(n, p)] = abs(fnp_prime);
            iartd[idx(n, p) + 1] -= phase_n_1[n];
        }
    }
    return iartd;
'';
\end{lstlisting}

\begin{lstlisting}
  // IARTD.cu
  // These are the associated CUDA Kernels for IARTD Calculations
__global__ void art_np_kernel(int height, int width, int n, int p,
                              unsigned char* image, float* dev_fnp_re,
                              float* dev_fnp_imag, int left_x, int bottom_y) {
    // thread values
    int thread_x = (blockIdx.x * blockDim.x) + threadIdx.x;
    int thread_y = (blockIdx.y * blockDim.y) + threadIdx.y;

    int x = thread_x + left_x;
    int y = thread_y + bottom_y;

    int orig_loc = x + width * y;

    if (x < width && y < height) {
        // Define some vectors that will be used to construct rho in polar
        // coords
        float x_vec = x - width / 2;
        float y_vec = y - height / 2;
        // We normalize rho to have a diameter equal to the width of the
        // image
        //  This prevents the corners from having some of the "basis"
        //  functions, but this is the way that the paper presents it
        float rho = sqrtf(x_vec * x_vec + y_vec * y_vec) / (width / 2);

        // Theta in polar coords based on where we are in the image
        float theta = atan2f(y_vec, x_vec);
        // We Are only looking at a normalized rho of 1
        // This is part of the integration
        if (rho <= 1) {
            // This is the R-cos functon that is used to derive some of the
            // angular invariance (Eq 7)
            float R = (n == 0) ? 1.0 : 2.0 * cosf(3.1415928 * n * rho);
            // This is defining A, which gives rotation invariance (Eq 6)
            thrust::complex<float> A =
                (1 / (2 * 3.1415928)) *
                exp(thrust::complex<float>(0.0, p * theta));
            // This is defining the integration over the whole image, and
            // constructiong the full value of F_np (Eq 4)
            thrust::complex<float> fnp_complex = image[orig_loc] * A * R * rho;
            atomicAdd(&dev_fnp_re[0], fnp_complex.real());
            atomicAdd(&dev_fnp_imag[0], fnp_complex.imag());
        }
    }
  }

std::complex<float> img_desc::art_n_p(int n, int p,
                                      gpu_cost_function::GPUImage* dev_image) {
    // Standard defintion for creating our work groups
    const int threads_per_block = 256;
    int* bounding_box = dev_image->GetBoundingBox();
    int left_x = max(bounding_box[0], 0);
    int bottom_y = max(bounding_box[1], 0);
    int right_x = min(bounding_box[2], width_ - 1);
    int top_y = min(bounding_box[3], height_ - 1);
    int diff_cropped_width = right_x - left_x - 1;
    int diff_cropped_height = top_y - bottom_y + 1;

    dim3 dim_grid_bounding_box =
        dim3(ceil(static_cast<float>(diff_cropped_width) /
                  sqrt(static_cast<float>(threads_per_block))),
             ceil(static_cast<float>(diff_cropped_height) /
                  sqrt(static_cast<float>(threads_per_block))));

    dim3 dim_block = dim3(ceil(sqrt(static_cast<float>(threads_per_block))),
                          ceil(sqrt(static_cast<float>(threads_per_block))));

    // Reset the variables that we are storing
    reset_vars<<<1, 1>>>(dev_Fnp_re, dev_Fnp_imag);
    // Run the kernel
    art_np_kernel<<<dim_grid_bounding_box, dim_block>>>(
        height_, width_, n, p, dev_image->GetDeviceImagePointer(), dev_Fnp_re,
        dev_Fnp_imag, left_x, bottom_y);

    // Copying everything back to host (CPU)
    cudaMemcpy(Fnp_re, dev_Fnp_re, sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(Fnp_imag, dev_Fnp_imag, sizeof(float), cudaMemcpyDeviceToHost);

    // Returning the value of the complex function that we have calculated.
    std::complex<float> fnp(Fnp_re[0], Fnp_imag[0]);
    return fnp;
};


\end{lstlisting}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Andrew_Jensen_Dissertation"
%%% End:
