\chapter{Correcting Symmetric Implant Ambiguity in Measuring Total Knee Arthroplasty Kinematics from Single-Plane Fluoroscopy}{\renewcommand*{\thefootnote}{\fnsymbol{footnote}}\footnotetext{Submitted for publication to the Journal of Biomechanics}



Measuring total knee arthroplasty (TKA) kinematics from fluoroscopic images has been an important contributor to knee implant design, post-operative assessment, and predictive modeling for wear and failure patterns for nearly 30 years \cite{banksRationaleResultsFixedBearing2019,banks2003HapPaul2004,freglyComputationalWearPrediction2005}.
However, the application of this technology has been limited to research use by the challenges of performing these measurements quickly and reliably, as they often require expensive equipment and time-consuming processes \cite{banksAccurateMeasurementThreedimensional1996,lafortuneThreedimensionalKinematicsHuman1992,zuffiModelbasedMethodReconstruction1999,mahfouzRobustMethodRegistration2003}.
Recent advancements in computer vision and machine learning have opened up the possibility of using a single fluoro-camera for fully autonomous TKA kinematic measurement \cite{brobergValidationMachineLearning2023,jensenJointTrackMachine2023}, making it more accessible and cost-effective for hospitals and clinics.
Nonetheless, this approach faces inherent limitations in accurately resolving orientation and location information using only one fluoro-camera view \cite{banksAccurateMeasurementThreedimensional1996,floodAutomatedRegistration3D2018,mahfouzRobustMethodRegistration2003,yamazakiImprovementDepthPosition2004,zuffiModelbasedMethodReconstruction1999}.

One of the key issues researchers encounter when dealing with imaging from a single camera arises from symmetric implant geometries.
Under a weak perspective/nearly orthographic projection paradigm \cite{szeliskiComputerVisionAlgorithms2022}, symmetric implants have distinct 3D orientations that produce nearly indistinguishable 2-dimensional projective geometry.  (\cref{fig:symmetry-trap-quadrants}).
Due to the model-image registration process relying on the information present in the 2D fluoroscopic image, this causes multiple local minima for many optimization algorithms.
Human-supervised kinematics measurements for TKA with symmetric tibial implants frequently rely on the relative location of the fibula in the fluoroscopic image to disambiguate difficult poses.
Unfortunately, fully autonomous solutions can’t utilize this reference point, causing difficulty in algorithmic implementation.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.78\textwidth]{~/figures/raster/sym-trap-quadrants-no-captions.png}
	\caption[Two examples of symmetric tibial poses]{Two examples of symmetry pose dyads with the correct (left) and symmetry trap (right) kinematics. For some orientations, the symmetry trap is obviously incorrect (top), and for others, it is more ambiguous (bottom).}
	\label{fig:symmetry-trap-quadrants}
\end{figure}


In this paper, we utilize 12,592 images from seven studies that utilized human-supervised TKA kinematics \cite{jennyRegistrationKneeKinematics2015,kefalaAssessmentKneeKinematics2017,okamotoVivoKneeKinematics2011,palm-vlasakMinimalVariationTop2022,scottCanTotalKnee2016,watanabeKneeKinematicsAnterior2013,watanabeInvivoKinematicsHighflex2016} to explore the potential of a novel method to classify and correct ``symmetry trap'' poses in single-plane TKA kinematic measurements.
The goal is to improve the accuracy and resolution of single-plane TKA measurements and enable more accessible and reliable kinematic data for orthopaedic applications.

This research paper will answer two key questions:  (1) How effectively can machine learning methods distinguish between ``true'' and 'symmetry trap' poses in cases involving symmetric implants? (2) How well can we systematically correct ``symmetry traps'' arising in kinematics sequences obtained from fluoroscopic images?

\section{Methods}
\subsection{Study Design and Sample}
In this paper, we are addressing the issue of “symmetry traps” in TKA kinematics measurements from single-plane fluoroscopy.
To do so, we take a data-driven approach.
We start with a collection of human-supervised “true” kinematic measurements, then determine the respective “symmetry trap” orientation using a novel algorithm.
Using this collection of both “true” and “symmetric” poses, we train eight different machine learning classifiers and evaluate their performance on both an internal and external test set.
However, for more ambiguous poses, it is typically the case that both the “symmetry trap” and “true” pose appear “true”.
To account for these, we then take the highest performing classifier and use it to establish an algorithm that takes as input a contiguous sequence of kinematics measurements, and imposes continuity constraints to fix “symmetry traps” in real-world data.
We report the performance of our cubic spline correction algorithm on a dataset of autonomously measured kinematics \cite{jensenJointTrackMachine2023}.
All programs were written in Python 3.10.

The dataset used for development of the methodology comprises seven total studies \cite{jennyRegistrationKneeKinematics2015,kefalaAssessmentKneeKinematics2017,okamotoVivoKneeKinematics2011,palm-vlasakMinimalVariationTop2022,scottCanTotalKnee2016,watanabeKneeKinematicsAnterior2013,watanabeInvivoKinematicsHighflex2016} comprising 12,592 images of radiographic data.
We completely withheld data from one study \cite{okamotoVivoKneeKinematics2011} during training to use later as an external test set.

\subsection{Determining Symmetric Orientations}
First, ``symmetry trap'' poses were identified from ``true'' poses through a novel algorithm devised to “flip” any given pose into its symmetric counterpart (\cref{fig:sym-flipper-alg}). The algorithm proceeds as follows:

\begin{enumerate}
	\item Determine the viewing ray from the object origin to the camera origin. Denote this as $\vec{v}$, where $||\vec{v}|| = 1$.
	\item Determine the symmetric axis of the object, $\vec{s}$, where $||\vec{s}|| = 1$. This symmetric axis is equivalent to the normal vector of the ``mirror plane'' of symmetry for the object.
	\item Determine the axis and angle of rotation between $\vec{s}$ and $\vec{v}$ and construct the equivalent rotation matrix.
	      \begin{enumerate}
		      \item The axis is the cross product, $\vec{m} = \vec{v} \times \vec{s}$.
		      \item The angle is the normalized dot product, $\psi = 2 \times \cos^{-1}\dfrac{\vec{v} \cdot \vec{s}}{||\vec{v}||||\vec{s}||}$
	      \end{enumerate}
	\item Apply body-centered rotation to the symmetric object about $\vec{m}$ by $\psi$.
\end{enumerate}

For each projective geometry, there exist exactly two poses (i.e., a dyad) that produce it, thus applying this algorithm twice will return the original pose.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{./figures/raster/symmetry_flipper.png}
	\caption{A visualization of the ``symmetry flipper'' algorithm that converts 3D orientations into their symmetric counterpart.}
	\label{fig:sym-flipper-alg}
\end{figure}

\subsection{Constructing Dataset}
We collected data from seven studies utilizing human supervised total knee arthroplasty kinematics, totaling approximately 12,592 individual images.
Anatomic tibiofemoral kinematics were stored as ``true'' poses.
The proposed algorithm was then applied to each image's tibial implant, generating corresponding “symmetric” poses, for a total of 25,184 samples (\cref{fig:sym-trap-dataset}).
Solid angle distances ($\psi$) obtained from the symmetry flipper algorithm were also stored, and the dataset was stratified based on the solid-angle value and split into 67\% training and 33\% testing sets for unbiased evaluation.
Data from one study \cite{okamotoVivoKneeKinematics2011} was completely withheld from training to be used as an external test set.

Thus, for each sample, the input was $\{\theta_{Int/Ext}, \theta_{Flx/Ext}, \theta_{Var/Val}, \psi\}$ with $\theta_{Int/Ext}, \theta_{Flx/Ext}, \theta_{Var/Val}$ representing anatomic internal/external rotation, flexion/extension, and varus/valgus respectively. The target was either `true' or `symmetry trap'.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.85\textwidth]{./figures/raster/symmetry-trap-dataset.png}
	\caption[A 3D scatter plot of our training data with `true' poses in orange and `symmetry trap' poses in blue]{A 3D scatter plot of our training data with `true' poses in orange and `symmetry trap' poses in blue. Of note, there are distinct regions of exclusively `symmetry trap' poses, while the region of predominately `true' poises (orange cylinder along Flexion/Extension axis) also has many `symmetry trap' poses.}
	\label{fig:sym-trap-dataset}
\end{figure}
\subsection{Classification Algorithms}
In this study, we implemented a selection of classification algorithms that use a range of different learning strategies (Table 1) from Scikit-learn \cite{scikit-learn} to classify any individual input pose as  ``true'' or ``symmetry trap''.
The chosen algorithms were K-Nearest-Neighbors (KNN) \cite{fixDiscriminatoryAnalysisNanparametric1951},
Support Vector Machines (SVM) \cite{cortesSupportvectorNetworks1995}, AdaBoost \cite{friedmanGreedyFunctionApproximation2001},
Histogram Gradient Boosting \cite{freundDecisionTheoreticGeneralizationOnLine1997},
Bagging Meta-Estimator \cite{breimanBaggingPredictors1996},
Stacked Generalization \cite{smythLinearlyCombiningDensity1999,wolpertStackedGeneralization1992},
and (majority) Voting Classifier.
The hyperparameters for each method were tuned using stratified 5-fold cross validation using grid-search to test all possible combinations.
Stacked generalization and the voting classifier used the tuned hyperparameters from each of the other estimators in their predictions.
We recorded sensitivity \cite{yerushalmyStatisticalProblemsAssessing1947}, specificity \cite{yerushalmyStatisticalProblemsAssessing1947}, accuracy \cite{internationalorganizationforstandardizationAccuracyTruenessPrecision2023},
and the F1 score \cite{tahaMetricsEvaluating3D2015} for each algorithm on both our internal and external test sets.
For the highest performing individual classifier on the internal test set, we also evaluated these metrics on stratified partitions of the internal test set to determine relative performance for different $\psi$ values.


\subsection{Fixing Incorrect Symmetry Trap Poses}
The primary objective of this study is to accurately measure the kinematics of all images in a TKA kinematics sequence and to robustly fix incorrect symmetry trap poses.
However, due to the inability to know a-priori which poses are “true” or in a “symmetry trap”, we must employ a separate procedure for systematically correcting individual images.
Thus, for every kinematics sequence we employ the following procedure:

\begin{enumerate}
	\item Determining Ambiguous and Non-ambiguous Poses for each image
	      \begin{enumerate}
		      \item For each image, we generate the `symmetric' pose corresponding to the input pose.
		      \item We run both the input and its symmetric pose through the highest performing classification algorithm.
		      \item If the input pose and its symmetric pose are differently labeled by the classifier (i.e., one is labeled ``true'', and the other ``symmetry trap''), take the pose labeled ``true'' as the actual pose.
		      \item If both poses return the same label (i.e., both ``true'' or both ``symmetry trap''), label the pose as “ambiguous” and move to step 2.
		      \item If no poses were labeled “ambiguous”, the procedure is finished.
	      \end{enumerate}
	\item Construction of 3-Dimensional cubic b-spline of the movement
	      \begin{enumerate}
		      \item A spline is individually fit through the flexion/extension, internal/external rotation, and adduction/abduction angles for all images that were {\bf not} labeled ambiguous.
	      \end{enumerate}
	\item Correcting Ambiguous Poses
	      \begin{enumerate}
		      \item We sample the cubic spline at image locations of ambiguous poses.
		      \item We compare the solid-angle difference between the input and symmetric pose to the pose at the sampled spline location.
		      \item Whichever pose is closer to the spline, we take this as the `true' pose.
	      \end{enumerate}
\end{enumerate}

To evaluate this procedure, we used a dataset of kinematics generated from a fully autonomous method \cite{jensenJointTrackMachine2023} to emulate the real-world use case of this algorithm as a post-processing operation.
This test set has robustly quantified the presence of “symmetry traps”, and so allows us to report sensitivity, specificity, accuracy, and the F1 measure of its ability to correct symmetry traps in a clinical setting.


\section{Results}
\subsection{Machine Learning Classification Performance}
Classification accuracy on the internal test set ranged from 87.8\% to 94.0\%, sensitivity ranged from 92.9\% to 94.7\%, and specificity ranged from 83.5\% to 93.6\% for all eight methods evaluated (\cref{tab:symtrap-ml-results}).
Stacked generalization and the voting classifier tended to outperform the other classifiers, as they both incorporated a combination of the other models in their decision-making.
For the external test set, accuracy ranged from 90.9\% to 94.2\%, sensitivity from 93.6\% to 97\%, and specificity from 88.4\% to 91.9\%  (Table 1).
The SVM with Radial Basis Function kernel had the highest accuracy and F1 score, at 94\% and 0.94 respectively. Stacked generalization performed slightly worse in the external test set.
Overall, the performance in the external test set was comparable to the performance on the internal test set, with some classifiers seeing decreased performance (K-Nearest-Neighbors, Stacked Generalization), and others seeing slightly increased performance (Support Vector Machines, AdaBoost, Voting Classifier).

% \begin{table}
% 	\centering
% 	\includegraphics*[width=0.95\linewidth]{./figures/raster/sym-trap-ML-table.png}
% \end{table}

\begin{landscape}
	\begin{table}[h]
		\caption{Machine learning classifier performance}
		\label{tab:symtrap-ml-results}
		\centering
		\begin{threeparttable}
			\begin{tabularx}{1.0\linewidth}{V{12cm}X X X X V{3cm} V{3cm} V{3cm}}
				\hline
				Classifier                                                   & Hyperparameters                       & Test Set & Accuracy (\%)      & Sensitivity (\%)   & Specificity (\%)   & F1-Score          \\ \hline
				\multirow{2}{*}{\parbox{3cm}{Support Vector Machine (RBF)}}  & \multirow{2}{*}{$C=1000$}             & Internal & $92.08$            & $94.75$            & $89.73$            & $0.92$            \\
				                                                             &                                       & External & ${94.21}$\tnote{b} & $97.12$            & $91.64$            & ${0.94}$\tnote{b} \\
				                                                             & \vspace{0.5cm}                                                                                                                      \\
				\multirow{2}{*}{\parbox{3cm}{Support Vector Machine (Poly)}} & \multirow{2}{*}{\parbox{3cm}{$C=1000$                                                                                               \\Poly Degree $=2$}}& Internal & $87.65$      & $92.54$         &  $83.8$                & $0.87$                          \\
				                                                             &                                       & External & $92.08$            & $96.58$            & $88.38$            & $0.92$            \\
				                                                             & \vspace{0.5cm}                                                                                                                      \\
				\multirow{2}{*}{\parbox{3cm}{K-Nearest-Neighbors}}           & \multirow{2}{*}{\parbox{3cm}{$N=4$                                                                                                  \\Minkowski Dist.}} & Internal & $93.12$     & $93.96$          & $92.32$                 &  $0.93$                          \\
				                                                             &                                       & External & $90.93$            & $93.62$            & $88.55$            & $0.91$            \\
				                                                             & \vspace{0.5cm}                                                                                                                      \\
				\multirow{2}{*}{\parbox{3cm}{AdaBoost}}                      & \multirow{2}{*}{$N=200$}              & Internal & $88.78$            & $91.08$            & $86.74$            & $0.88$            \\
				                                                             &                                       & External & $92.86$            & ${97.23}$\tnote{b} & $89.22$            & $0.93$            \\
				                                                             & \vspace{0.5cm}                                                                                                                      \\
				\multirow{2}{*}{\parbox{3cm}{Histogram Gradient Boosting}}   & \multirow{2}{*}{$L.R. = 0.1$}         & Internal & $93.11$            & $94.98$            & $91.4$             & $0.93$            \\
				                                                             &                                       & External & $93.24$            & $96.67$            & $90.29$            & $0.93$            \\
				                                                             & \vspace{0.5cm}                                                                                                                      \\
				\multirow{2}{*}{\parbox{3cm}{Bagging Estimator}}             & \multirow{2}{*}{$N=500$}              & Internal & $93.32$            & $94.28$            & $92.41$            & $0.93$            \\
				                                                             &                                       & External & $93.82$            & $95.95$            & ${91.88}$\tnote{b} & $0.94$            \\
				                                                             & \vspace{0.5cm}                                                                                                                      \\
				\multirow{2}{*}{\parbox{3cm}{Stacked Generalization}}        & \multirow{2}{*}{N/A}                  & Internal & ${94.3}$\tnote{a}  & $94.78$            & ${93.84}$\tnote{a} & ${0.94}$\tnote{a} \\
				                                                             &                                       & External & $92.86$            & $94.94$            & $90.96$            & $0.93$            \\
				                                                             & \vspace{0.5cm}                                                                                                                      \\
				\multirow{2}{*}{\parbox{3cm}{Majority Voting Classifier}}    & \multirow{2}{*}{N/A}                  & Internal & $92.63$            & ${95.85}$\tnote{a} & $89.86$            & $0.92$            \\
				                                                             &                                       & External & $93.34$            & $96.87$            & $90.31$            & $0.93$            \\ \hline
			\end{tabularx}
			\begin{tablenotes}
				\item[a] Highest value in the internal test set.
				\item[b] Highest value in the external test set.
			\end{tablenotes}
		\end{threeparttable}
	\end{table}
	\begin{table}[h]
		\caption{Stratified-$\psi$ test set stacked generalization classification performance}
		\label{tab:strat-psi}
		\centering
		\begin{tabularx}{1.0\linewidth}{XXXXXX}
			\hline
			Psi Range        & Sample Size & Accuracy (\%) & Sensitivity (\%) & Specificity (\%) & F1-Score \\
			\hline
			$0-5^{\circ}$    & 488         & $71.04$       & $71.43$          & $70.7$           & $0.69$   \\
			$5-10^{{\circ}}$ & 1132        & $88.21$       & $90.53$          & $85.98$          & $0.88$   \\
			$10-15^{\circ}$  & 1224        & $93.0$        & $92.78$          & $93.2$           & $0.93$   \\
			$15-20^{\circ}$  & 1107        & $96.13$       & $96.97$          & $95.31$          & $0.96$   \\
			$>20^{\circ}$    & 3568        & $98.28$       & $98.32$          & $98.24$          & $0.98$   \\
			\hline
		\end{tabularx}
	\end{table}
\end{landscape}

\subsection{Stratified \texorpdfstring{$\psi$}{ψ} Test Set Performance}
As the value of $\psi$ increases, the performance of stacked generalization classification increases (\cref{tab:strat-psi}).
At values closer to a pure-lateral view (~$0^{\circ}$), accuracy is roughly 71\%, and once the implants exceed $5^{\circ}$ off-lateral, accuracy increases to above 88\%.

% \begin{figure}
% 	\centering
% 	\includegraphics*[width=0.95\linewidth]{./figures/raster/stratified-psi-ML-table.png}
% \end{figure}


\subsection{Symmetry Trap Correction Performance}
When applying the procedure to correct symmetry traps, we achieve an accuracy of
91.1\%, a sensitivity of 67.4\% and a specificity of 94\% on an external test
set. The average “distance to symmetric pose” ($\psi$) was $16.6^{\circ}$ for
the images correctly classified, and $7.12^{\circ}$ for the images incorrectly
classified. For our initial internal test set, the distance to the symmetric
pose was $25.6^{\circ}$ for images correctly classified, and $9.65^{\circ}$ for
incorrectly classified images. Thus, the incorrect images are closer to a true
lateral single-plane image, meaning the difference between the symmetric dyad of
poses is smaller.

\section{Discussion}
Orthopaedic surgeons desire a practical method to clinically quantify TKA
kinematics for post-operative assessment or exploration of unsatisfactory
outcomes \cite{banksWhatPostoperativeOutcome2017}. To date, manual methods for
quantifying TKA kinematics from single-plane radiographic images have been too
time-consuming for clinical use. However, the application of machine learning
methods has significantly enhanced their convenience
\cite{jensenJointTrackMachine2023}. Modern autonomous kinematics measurement
software, implemented on a modest laptop with a dedicated graphics card,
demonstrates efficient processing capabilities, requiring approximately 5–15
seconds for analyzing a single image \cite{jensenJointTrackMachine2023}. A
critical step in making this technology ready for clinical use is to address the
pose ambiguity issue that arises from single-camera views of symmetric tibial
implant components in a time-efficient manner. We found that several machine
learning algorithms were able to robustly identify and correct symmetric
erroneous poses with greater than 90\% accuracy, making these measurement
methods sufficiently autonomous and robust to be considered for clinical use.

We demonstrated that classical machine learning techniques are extremely adept
at handling the classification of “true” and “symmetry trap” TKA kinematics
measurements from single-plane fluoroscopy. As a first study in this domain,
benchmark comparisons with other metrics, datasets, or algorithms are not
available. However, an accuracy level of at least 87\% establishes a useful
performance benchmark in terms of clinical applications of these methods and for
future research in this field. As expected, stacked generalization
\cite{wolpertStackedGeneralization1992,smythLinearlyCombiningDensity1999}, where
“the worst possible performance is the highest of any individual estimator” held
true for our internal test set, and outperformed all other algorithms.
Interestingly, the same trend did not hold for our external test set, which
contained kinematics from patients, imaging machines, and surgeons that were
completely withheld from training. In our external test set, SVMs with radial
basis function kernel seems to outperform the other classifiers.


Our symmetry trap correction procedure has strong performance in the accuracy
($>91\%$) and specificity ($>94\%$). We note decreased sensitivity related to
decreasing values of our $\psi$ term, consistent with intuition: when two
possible poses are closer together, they are harder to disambiguate. When
performance is measured based on $\psi$ values greater than 5 or 10 degrees, we
see that our symmetry trap correction algorithm boasts notably increased
performance (Table 2). Previous work has also shown an increase in the
prevalence of symmetry traps in low-$\psi$ poses
\cite{jensenJointTrackMachine2023}. We also note that in real-world data, we
have imbalanced class representation because most symmetry traps are caught in
the initial kinematics measurement, leaving only the most difficult cases for
this algorithm to correct. These reasons both culminate with an expected
decreased sensitivity when trying to correct symmetry trap poses. Future work
might be able to address this problem using time-series machine learning
algorithms like recurrent neural networks
\cite{hochreiterLongShortTermMemory1997} and transformers
\cite{vaswaniAttentionAllYou2017} to isolate specific images that have fallen
into symmetry traps based on local and global relationships with other
kinematics in that particular sequence. Additionally, 3D geometries could be
used to impose penetration and separation penalties that are further used to
refine ambiguous poses.


The performance of autonomous measurements of single-plane fluoroscopy
kinematics measurements already achieve clinically acceptable levels of error.
Despite this, measurement errors still occur and roughly 67\% of images that are
incorrectly optimized fall into symmetry traps
\cite{jensenJointTrackMachine2023,brobergValidationMachineLearning2023}. With
the techniques proposed in this work, we can drastically improve the overall
performance of autonomous single-plane TKA kinematics measurements and remove
the largest source of inaccuracy.


Stratifying measurement performance by (Table 2), it is clear that the joint
viewing perspective is a dominant contributor to pose ambiguities. This suggests
a modification to imaging protocols for measuring TKA kinematics is needed to
minimize the number of images that fall into the “ambiguous zone”. Instead of
“pure lateral” imaging that attempts to perfectly center the knee in the image
frame, we suggest positioning the center of the knee 1–2 inches away from the
center of projection, and to rotate the plane of the observed leg by 5 degrees
from parallel with the image detector. These small adjustments to the
image/patient alignment would cause all kinematics to have $\psi > 5^{\circ}$,
which improves classification accuracy by at least 20\%. Similar imaging
protocol improvements have been proposed for bi-plane
RSA\cite{niesenReorientingTibialBaseplate2020}.

This study has some limitations. In this study, we use human-supervised
kinematics from single-plane images, where there is always the possibility that
the human operator fell into error when distinguishing between “true” and
“symmetry trap” poses in a sequence of images. This can occur due to a myriad of
factors, including low image quality and near-lateral imaging angles. To
mitigate this, validated ground truth kinematics would be necessary to create a
robust algorithm without any errors in the training data.

The correction algorithm recommended in this study assumes that the femoral
implant has been properly registered with correct kinematics. While this is
typically a good assumption because most femoral implants are asymmetric, and
have an extremely low likelihood of falling into their own “symmetry traps”
\cite{jensenJointTrackMachine2023}, improper registration of both a symmetric
tibial implant and symmetric femoral implant would decrease the feasibility of
the proposed procedure.

\section{Concluding Remarks}
This study presents a novel method of overcoming one of the most pernicious
issues facing researchers measuring TKA kinematics from single-plane
fluoroscopy. Our algorithm combines 3 elements (1) calculating “symmetry trap”
poses from “true” poses, (2) classical machine learning algorithms to classify a
single pose as “true” or “symmetry trap” and (3) a procedure that applies those
machine learning outputs to a contiguous sequence of kinematics to construct a
spline and correct symmetry traps in real-world data. Additionally, the
“symmetric flipper” algorithm provides a strong foundational approach for any
symmetric object in a weak-perspective paradigm and can drastically reduce the
complexity of many optimization algorithms by reducing the search space. Because
this algorithm is model-agnostic, it could be used in many applications where
orientation ambiguities arise. Our results also suggest slight alterations to
imaging angles, which could reduce errors due to ambiguous poses resulting from
symmetrical tibial implants. We believe that this method, along with recent
advancements in autonomous kinematics measurements, will soon make it practical
to perform dynamic TKA kinematics measurements in a clinical setting without the
inherent limitations of single-plane radiographic imaging.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Andrew_Jensen_Dissertation"
%%% End:
