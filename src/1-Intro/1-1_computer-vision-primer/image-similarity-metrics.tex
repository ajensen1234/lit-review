One of the key components in model image registration is image similarity, which entails evaluating the degree of correspondence between a user-generated synthetic image and an actual fluoroscopic image.
The selection of a similarity metric is influenced by various factors, such as the pre-existing knowledge of implant/bone geometry and the understanding of image quality and contrast.
Broadly, there are two classes of image similarity when performing model-image registration: intensity-based and feature based.

\subsubsection{Intensity based}
\label{sec:img-sim-intensity}
Intensity-based measures rely on the specific pixel values to quantify the difference between two images.
These measures can apply to the global image similarity or to specific regions of interest within the images.

A canonical difference between two images would be the p-norm separating them (\cref{eq:p-norm}), which computes the p-norm difference between corresponding pixel pairs across the two images.
Common p-norms are the $L_1$ norm (\emph{absolute intensity differences} or \emph{mean absolute difference}) \cite{kanadeStereoMatchingAlgorithm1994} ($p=1$) and the $L_{2}$ norm, or Euclidean norm (\emph{squared intensity differences} or \emph{mean squared difference}) \cite{hannahComputerMatchingAreas1977}($p=2$).

\begin{equation}
    \|A-B\|_{p} = (\sum_{x=0}^{w}\sum_{y=0}^{h}|a_{xy}-b_{xy}|^{p})^{\frac{1}{p}}
    \label{eq:p-norm}
\end{equation}

In Equation \cref{eq:p-norm}, $A$ and $B$ are the two images being compared, $w$ and $h$ are the width and height of the images, and $a_{xy}$ and $b_{xy}$ are the intensity values at pixel $(x,y)$ in the two images, respectively.

While p-norm measures are conceptually straightforward to apply, their primary drawback is the absence of spatial information.
For instance, an image that has been altered by a linear transformation might not achieve a favorable score using a p-norm, even though the two images differ only by a minor shift, scale, or rotation.
To mitigate this limitation, cross-correlation, or the sliding dot product between images, can be employed \cite{bendatRandomDataAnalysis2010,hannahComputerMatchingAreas1977} (\cref{eq:xcorr}).
This technique, when combined with projective geometry, aids in identifying regions of interest within a model-based registration framework.

\begin{equation}
    \begin{aligned}
        (A \star B)[x,y] &= E[A_{xy} \cdot B_{x + \tau_x,y+\tau_y}] \\
        &= \sum_{\tau_x=-\infty}^{\infty}\sum_{\tau_y=-\inf}^{\infty}a_{xy}b_{x + \tau_x,y + \tau_y}
    \end{aligned}
    \label{eq:xcorr}
\end{equation}

Cross correlation efficiently identifies regions within each image that demonstrate similarity, resulting in higher function values at those pixel locations.
This is very similar to how a convolution operation works.
The implementation of normalized cross-correlation (\cref{eq:norm-xcorr}) further refines this process by attenuating noise inherent in the original images.
This normalization ensures that the correlation measure is predominantly influenced by the structural correspondence between the images, enhancing the precision of similarity assessments within the framework of image registration, independent of variations in illumination or exposure.

\begin{equation}
    \begin{aligned}
        \text{normalized cross correlation}(A,B) &= \frac{A \star B}{(A \star A)(B \star B)}
    \end{aligned}\label{eq:norm-xcorr}
\end{equation}

\subsubsection{Feature based}
\label{sec:img-sim-feature}
Feature-based image similarity metrics utilize methods that identify key features within images, employing these features to gauge differences between two images.
These approaches typically require a feature-extraction phase, during which the relevant features are identified and quantified for later analysis.
The two main classes of features are \emph{keypoints} and \emph{edges}.
One of the simplest forms of keypoint detection is hand-crafting a convolution kernel containing the desired feature.
The convolution operation will yield regions in the image containing that particular feature, which then become keypoints.
With keypoints detected in the input image, one could determine the error of the current pose estimate by taking the Euclidean distance between all image keypoints and all projected keypoints \cite{burtonAutomaticTrackingHealthy2021} (\cref{eq:kp-error}).
With a-priori information about the keypoints, weights could be assigned to each keypoint in order to emphasize specific regions on the image and the model (\cref{eq:wkp-error}).

\begin{equation}
    \begin{aligned}
        \text{Keypoint Error}= (\sum_{i = 0}^{N}(KP_{image,i} - KP_{proj,i})^2)^{\frac{1}{2}}
    \end{aligned}
    \label{eq:kp-error}
\end{equation}

\begin{equation}
    \begin{aligned}
        \text{Weighted Keypoint Error} = (\sum_{i = 0}^{N}w_{i}(KP_{image,i} - KP_{proj,i})^2)^{\frac{1}{2}}
    \end{aligned}
    \label{eq:wkp-error}
\end{equation}

Keypoints are particularly useful when there are invariant features in images and 3D models, like morphological aspects of bones.
However, if these features cannot be detected, alternative measures must be utilized.

\paragraph*{Edges as features}
Edges are a natural choice of feature when determining image similarity.
Similar to intensity-based image similarity, the similarity between the contours of two images offers a promising metric for determining the overall similarity between two images.
In model-image registration, the contours of the input image and the projected model can easily be compared, which presents a reliable scheme for measuring their similarity.
When the edges are aligned, we say that the model is \textit{properly registered} to the image.
However, how can we determine when the edges are aligned?

As always, the simplest approach is to take the p-norm between the model and image contours (\cref{eq:p-norm}), where instead of taking the difference between the two original images, one is taking the difference of the edges of the images.
This function will be minimized when there is complete overlap between image and model contours.

\paragraph{Drawback of feature based similarity metrics}
While they can be much more informative and specific, the main drawback of feature based image similarity metrics is the need for feature selection and extraction from the input image.
Historically, this required an immense amount of domain-specific knowledge, and the image processing used to extract those features was often relatively limited in scope.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../../Andrew_Jensen_Dissertation"
%%% End:
