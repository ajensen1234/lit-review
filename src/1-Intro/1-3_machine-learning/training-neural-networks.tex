\subsection{Training Neural Networks}

The power of neural networks is that they are able to ``learn'' incredibly complex mappings from a set of training data. But how \emph{exactly} do they learn?

\subsubsection{Neural Network Cost Functions}

The first step in determine how a machine learns is knowing how correct an output is. The {\bf cost function} is exactly the error metric that determines the wrongness of the neural network's output. It's this function that the neural network is attempting to minimize as it updates its weights.

The choice of cost function is dependent on the task at hand. For a regression problem, a standard mean-squared-error is typically employed 

\begin{center}
    \Huge{PUT TABLE WITH SOME COST FUNCTIONS HERE}
\end{center}

\subsubsection{Optimizing and Updating Weights}

For convex problems, there often exists a closed-form solution for the parameters that minimize a given objective function. However, neural networks are highly non-linear and non-convex, forcing researcher to employ different methods of updating parameter values in a direction of minimizing the objective function. Mathematically, this means that we are determining the local gradient of the cost function with respect to each of the parameters, and updating the parameters in the direction of the steepest negative gradient.

If $w$ is the collection of all learnable parameters, then we are trying to minimize the error between the $y = \text{model}(x)$ and our true value, $t$. In equation \ref{eq:objective-fn}, $F(\cdot)$ is our objective function, $y$ is our model output, and $t$ is the true label.

\begin{equation}
    J(\text{model}) = F(y,t)
    \label{eq:objective-fn}
\end{equation}

The local gradient of the objective space with respect to the model weights can be used to determine the direction of movement. We also want to add a learning rate ($\eta$) to control how much each of the weights is updated in the desired direction (Eq. \ref{eq:grad-descent}). This is known as gradient descent.

\begin{equation}
    \begin{aligned}
        w^{(j+1)} &= w^{(j)} + \Delta w\\
        &\text{where} \\
        \Delta w &= -\eta \nabla J(w^{(j)})
    \end{aligned}
    \label{eq:grad-descent}
\end{equation}

In order to determine the gradient at a specific weight, we must consider all the operations that occur between the weight and the final output. This can be shown visually quite simply (Fig. XYZ). Calculating the gradient at a specific weight involves a simple chain-rule operation (Eq. \ref{eq:chain-rule}). The chain rule here is based on determining the local gradient at a weight only passing through a single layer. This process is known as backpropagation, and it ubiqutious for training neural networks \cite{rumelhartLearningRepresentationsBackpropagating1986}.

\begin{center}
    \Large{GRGAPH NN FOR GRAD DESCENT}
\end{center}

\begin{equation}
    \frac{\partial J}{\partial w} = \frac{\partial J}{\partial e}\frac{\partial e}{\partial \phi}\frac{\partial \phi}{\partial v} \frac{\partial v}{\partial w}
    \label{eq:chain-rule}
\end{equation}

One of the main limitations of gradient descent is the need to tune the learning rate. To small a value will cause a network to train extremely slowly and almost always get stuck in local minima. Too large a learning rate can cause the network to ``bounce out'' of the global minima due to the rate of change being too large. Hyperparameter tuning can be a difficult process without a defined methodology beyond trial and error. Some groups have proposed different methods of incorporating dynamically changing update rules in order to incorporate physical properties into the network training stage. 