\subsection{Training Neural Networks}

The power of neural networks is that they are able to ``learn'' incredibly complex mappings from a set of training data.
However, the specific mechanisms underlying this automated learning warrant further explanation.

\subsubsection{Neural network cost functions}

Defining appropriate metrics constitutes an initial step in refining machine learning systems.
This metric can encode specific insights about the task at hand, and it can also be used to promote a specific structure in the network itself like parsimony or sparsity.

Cost functions explicitly encode these metrics for optimization by learning algorithms.
Regression tasks often minimize distance metrics like mean absolute error, while classifications primarily utilize cross-entropy or log likelihoods \cite{paszkePyTorchImperativeStyle2019}.
The choice of cost function thereby specifies insights to extract from data.

\subsubsection{Optimizing and updating weights}

For convex problems, there often exists a closed-form solution for the parameters that minimize a given objective function.
However, neural networks are highly non-linear and non-convex, forcing researchers to employ different methods of updating parameter values in a direction of minimizing the objective function.
Backpropagation has emerged as the ubiquitous method for updating these highly non-linear systems \cite{rumelhartLearningRepresentationsBackpropagating1986}.
This utilizes gradient descent (\cref{eq:grad-descent}) and the chain rule (\cref{eq:chain-rule}) in order to calculate the affect that each node has on the final output, and update it in the direction that maximally minimizes the current error.
Researchers can control how aggressive the update is by changing the learning rate, $\eta$.

Whereas convex functions typically enable a closed-form solution \cite{boydConvexOptimization2004} for solving the parameters, neural networks are highly nonlinear and nonconvex, necessitating iterative methods for determining solution minima.
Backpropagation has emerged as the ubiquitous method for updating these highly non-linear systems \cite{rumelhartLearningRepresentationsBackpropagating1986}.
This employs gradient descent (\cref{eq:grad-descent}) and the chain rule (\cref{eq:chain-rule}) to calculate each node's contribution to output error.
Learning rates ($\eta$) control step sizes.

\begin{equation}
    \begin{aligned}
        w^{(j+1)} &= w^{(j)} + \Delta w\\
        &\text{where} \\
        \Delta w &= -\eta \nabla J(w^{(j)})
    \end{aligned}
    \label{eq:grad-descent}
\end{equation}

\begin{equation}
    \frac{\partial J}{\partial w} = \frac{\partial J}{\partial e}\frac{\partial e}{\partial \phi}\frac{\partial \phi}{\partial v} \frac{\partial v}{\partial w}
    \label{eq:chain-rule}
\end{equation}

One of the main limitations of gradient descent and backpropagation is the need to tune the learning rate.
Too small a learning rate will result in extremely slow training of the network and almost always get stuck in local minima.
Too large a learning rate can cause the network to ``bounce out'' of the global minima due to the rate of change being too large.
Hyperparameter tuning often presents challenges, frequently relying on methodologies that extend beyond simple trial and error.
Some groups have proposed different methods of incorporating dynamically changing update rules in order to incorporate physical properties into the network training stage.
The most common is Adaptive Moment Estimation \cite{kingmaAdamMethodStochastic2017}, which incorporates Root Mean Squared Propagation \cite{hinton2012neural} and momentum learning.

A major limitation to naive backpropogation with gradient descent is the need to manually tune the learning rate.
Excessively small rates slow training and risk local minima,  and large rates can cause instability.
Adaptive methods like Adam \cite{kingmaAdamMethodStochastic2017} and RMSprop \cite{hinton2012neural} incorporate momentum and parameter-specific tuning to automate convergence.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../../Andrew_Jensen_Dissertation"
%%% End:
