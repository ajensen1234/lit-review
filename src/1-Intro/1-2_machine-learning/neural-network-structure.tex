\subsection{Neural Network Structure}
The architecture of neural networks generally comprises the same fundamental components, which can be combined in various configurations based on the desired complexity and performance of the model.

\subsubsection{Neural network building blocks}
In general, neural networks are formed by an integration of foundational units, enabling the development of increasingly intricate architectures with enhanced capabilities.
However, an understanding of the underlying ``atoms'' constituting neural networks remains essential.

The fundamental unit of a neural network is the perceptron (or neuron) (\cref{fig:neuron}), comprising a summation of inputs each multiplied by their respective weights, a bias term, and typically a non-linear activation function.

\begin{figure}[h!]
    \begin{center}
        {\includegraphics[width=0.55\linewidth]{figs/background/png/neuron.png}}
    \end{center}
    \caption{A schematic representing a single neuron that receives $n$ inputs and applies $\phi$ as an activation function.}
    \label{fig:neuron}
\end{figure}

\begin{equation}
    y = \phi(\sum_{i=1}^{n}a_i w_i + b)
    \label{eq:neuron}
\end{equation}

Activation functions constitute a vital aspect of neural networks, allowing for non-linearity that enables learning of intricate representations of input data.
Without activation functions, neural networks would only capture linear relationships between inputs and outputs, which is insufficient for many real-world problems.
The optimal activation function is problem- and architecture-specific, with functions introducing varying degrees of non-linearity and computational efficiency.
A list of common activation functions is provided in \Cref{tab:activation-functions}.

\begin{table}[H]
    \caption{A list of activation functions and their corresponding mathematical formula} \label{tab:activation-functions}
    \begin{tabularx}{\columnwidth}{|X|X|X|}
        \hline
        {\bf Activation Function} & {\bf Equation} \\ \hline 
        Linear & $\phi(x) = x$ \\\hline
        Sigmoid & $\phi(x) = \frac{e^{x}}{1 + e^{x}}$\\ \hline 
        Hyperbolic Tangent & $\phi(x) = \frac{e^x - e^{-x}}{e^{x} + e^{-x}}$\\ \hline 
        Rectified Linear Unit (ReLU) & $\phi(x) = \text{max}(0,x)$\\ \hline 
        Leaky ReLU & $\phi(x) = \text{max}(0.1x,x)$ \\ \hline 
    \end{tabularx}
\end{table}

\subsubsection{Fully connected network}
The fully connected network, also known as the multi-layer perceptron, is a basic type of neural network that utilizes the neurons previously discussed as building blocks.
Its schematic representation is a familiar image to many when considering neural networks.
Stacking neurons' operations enables formulation of single-layer equations as matrix multiplications (\cref{eq:fcn}).
In this equation, $W$ represents the collection of weights for each neuron, $a$ represents the input, $b$ represents the bias, and $\phi$ represents the activation function.

A key strength of this network type is its utilization of non-linear activation functions, enabling the modeling of complex, non-linear relationships in data.
A well-chosen activation function can greatly impact the network's performance and ability to achieve specific tasks.
In binary classification contexts, sigmoid activation functions are often effective as output layers, constraining predictions between 0 and 1, interpretable as probabilities.
Rectified linear unit (ReLU) functions frequently optimize hidden layer efficiency.
When operating upon data following gaussian distributions, the hyperbolic tangent (tanh) activation can better match associated non-linearities.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/background/png/fcn.png}
    \caption[A basic fully connected network with a single hidden layer]{A basic fully connected network with a single hidden layer. Each of the neurons are exactly the same as the neurons shown in figure \ref{fig:neuron}}
    \label{fig:fcn}
\end{figure}

\begin{equation}
    y = \phi(W^{T}a + b)
    \label{eq:fcn}
\end{equation}

Additional layers can be added by taking the output of one layer and sending that into the input of the next. Increasingly complex mappings can be generated by increasing either the size or the number of hidden layers in a network.

\begin{equation}
    y = \phi_2 (W_2^{T}(\phi_1(W_1^{T}a + b_1) + b_2))
    \label{eq:2-layer-fcn}
\end{equation}

One of the main limitations of fully-connected networks is exponential increase in computational complexity as the input grows.
For a standard $1024 \times 1024$ image, there are roughly 1 million input nodes, which can lead to hundreds of millions of parameters that need to be learned.
Different neural network architectures have been created that employ standard image processing techniques, overcoming many of these limitations.

\subsubsection{Convolutional neural networks}

Alex Krizhevsky significantly renewed interest in deep learning in 2012 by employing a convolutional neural network to win the ImageNet challenge \cite{russakovskyImageNetLargeScale2015} by more than 10 points over second place.
Since then, neural networks have found their way into many different computer vision tasks, including those in the medical field.
The power of convolutional neural networks lay in their ability to autonomously extract latent features from images useful for many processing and analysis tasks.
The medical field has used them to segment and classify different bones, structures, and pathologies from a wide array of imaging modalities (CT, x-ray, MRI, etc).
In most cases, it can completely remove the need for human supervision in many repetitive image processing tasks.

Convolutional neural networks (CNN) utilize the convolution operation (\cref{eq:convolution}) in order to both reduce the size and complexity of the network and capture spatial information present in images.
In the same way that the matrix $W$ in the fully connected network is a learnable parameter in a FCN, the individual kernels are the learnable parameters in a convolution operation.
In practice, with the correct cost function and optimization routine, this allows each of the kernels to learn the latent structure in the image and make connections between those structures.
At a high level, these kernels often represent feature extractors for edges, lines, corners, curves, and other geometric primitives.
However, as one traverses deeper into the network, the combinations of these features are often incomprehensible to a human (\cref{fig:conv-layers}).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/background/png/conv-layers.png}
    \caption[An example of extracted features from a convolutional neural network]{An example of extracted features from a convolutional neural network. As shown, the deeper into the network one explores, the combination of core features start to create higher level features that might represent the shape of a specific animal \cite{schulzDeepLearningLayerWise2012}.}
    \label{fig:conv-layers}
\end{figure}

Typically, these networks will downsample the image to capture the most salient information, then upsample to regenerate the features from the underlying latent representations.
This network architecture has been popularized by the U-Net, which is known as a standard autoencoder (Fig. \ref{fig:unet}).
Each of the layers is composed of a collection of convolution kernels, and the downsampling occurs based on the stride or size of the kernel.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/background/png/u-net.png}
    \caption[A standard U-Net architecture for a convolutional neural network]{A standard U-Net architecture for a convolutional neural network. This architecture is the most common form of network used for biomedical image analysis and processing.}
    \label{fig:unet}
  \end{figure}


CNN architecture can be altered by changing the behavior and structure of the underlying kernels, namely size, stride, and shape.
Stride controls the discrete steps the kernel takes as it is moving across the input image and can be used to downsample more aggressively.
An atrous convolution involves internally padding the convolution entries with zeros. This also has the effect of more aggressively downsizing an image and capturing a larger region around the center pixel.

A convolutional neural network can have an additional fully connected network appended to the output in order to learn a mapping that involves a linear combination of the extracted features from the convolution operations.
This is often used when a CNN is applied to a classification task, where the total number of outputs of the final FCN is the number of classes.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../../Andrew_Jensen_Dissertation"
%%% End:
